{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHEq6dvlJlkA",
        "outputId": "3ea9f9c0-10c7-4f03-9c04-fbf3cb31079a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing omp_stats.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile omp_stats.cpp\n",
        "#include <iostream>        // Я подключаю ввод-вывод\n",
        "#include <vector>          // Я использую vector\n",
        "#include <omp.h>           // Я подключаю OpenMP\n",
        "#include <cmath>           // Я подключаю sqrt()\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "int main() {\n",
        "    const int N = 10000000;               // Я задаю размер массива\n",
        "    vector<double> data(N, 1.0);          // Я создаю массив из единиц\n",
        "\n",
        "    double t_start = omp_get_wtime();     // Я начинаю общий замер времени\n",
        "\n",
        "    double sum = 0.0;                     // Я объявляю сумму\n",
        "    #pragma omp parallel for reduction(+:sum)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        sum += data[i];                   // Я считаю сумму параллельно\n",
        "    }\n",
        "\n",
        "    double mean = sum / N;                // Я считаю среднее\n",
        "\n",
        "    double variance = 0.0;                // Я объявляю дисперсию\n",
        "    #pragma omp parallel for reduction(+:variance)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        variance += (data[i] - mean) * (data[i] - mean); // Я считаю дисперсию\n",
        "    }\n",
        "    variance /= N;\n",
        "\n",
        "    double t_end = omp_get_wtime();        // Я заканчиваю замер времени\n",
        "\n",
        "    cout << \"Threads: \" << omp_get_max_threads() << endl;\n",
        "    cout << \"Mean: \" << mean << endl;\n",
        "    cout << \"Variance: \" << variance << endl;\n",
        "    cout << \"Time: \" << (t_end - t_start) << \" s\" << endl;\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ omp_stats.cpp -fopenmp -O2 -o omp_stats\n",
        "!./omp_stats\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nivIEsn5Nbnv",
        "outputId": "c06edb58-77f2-4541-9431-966873a4beeb"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threads: 2\n",
            "Mean: 1\n",
            "Variance: 0\n",
            "Time: 0.0149982 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile memory_access.cu\n",
        "#include <iostream>            // Я подключаю ввод-вывод\n",
        "#include <cuda_runtime.h>      // Я подключаю CUDA\n",
        "\n",
        "__global__ void coalesced(float* data) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x; // Я обращаюсь к памяти линейно\n",
        "    if (i < 1024*1024) data[i] *= 2.0f;\n",
        "}\n",
        "\n",
        "__global__ void non_coalesced(float* data) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx = (i * 32) % (1024*1024);               // Я создаю разрозненный доступ\n",
        "    data[idx] *= 2.0f;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1024 * 1024;\n",
        "    size_t bytes = N * sizeof(float);\n",
        "\n",
        "    float* h = new float[N];\n",
        "    for (int i = 0; i < N; i++) h[i] = 1.0f;\n",
        "\n",
        "    float* d;\n",
        "    cudaMalloc(&d, bytes);\n",
        "    cudaMemcpy(d, h, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    dim3 threads(256);\n",
        "    dim3 blocks((N + 255) / 256);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    coalesced<<<blocks, threads>>>(d);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float t1;\n",
        "    cudaEventElapsedTime(&t1, start, stop);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    non_coalesced<<<blocks, threads>>>(d);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float t2;\n",
        "    cudaEventElapsedTime(&t2, start, stop);\n",
        "\n",
        "    std::cout << \"Coalesced time: \" << t1 << \" ms\\n\";\n",
        "    std::cout << \"Non-coalesced time: \" << t2 << \" ms\\n\";\n",
        "\n",
        "    cudaFree(d);\n",
        "    delete[] h;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBAkeD3eVmAB",
        "outputId": "e5e69aa5-cd5d-4977-f4e1-0859fb486937"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing memory_access.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc memory_access.cu -o memory_access -gencode arch=compute_75,code=sm_75\n",
        "!./memory_access\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql0fApXPVqnT",
        "outputId": "54e635d9-13f9-47be-a975-7a2f80227253"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coalesced time: 0.163968 ms\n",
            "Non-coalesced time: 0.287648 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile omp_profile.cpp\n",
        "#include <iostream>                                   // Я подключаю ввод-вывод\n",
        "#include <vector>                                     // Я подключаю vector для массива\n",
        "#include <cmath>                                      // Я подключаю sqrt() для вычислений\n",
        "#include <omp.h>                                      // Я подключаю OpenMP и omp_get_wtime()\n",
        "\n",
        "using namespace std;                                  // Я использую стандартное пространство имён\n",
        "\n",
        "int main() {                                          // Я начинаю программу\n",
        "    const int N = 10000000;                           // Я задаю размер массива (10 млн)\n",
        "    vector<double> data(N, 1.0);                      // Я создаю массив и заполняю его единицами\n",
        "\n",
        "    double t0 = omp_get_wtime();                      // Я фиксирую старт общего времени\n",
        "\n",
        "    // -------- последовательная часть (инициализация уже сделана выше) --------\n",
        "    double t_seq_start = omp_get_wtime();             // Я фиксирую время начала условно последовательной части\n",
        "    double dummy = 0.0;                               // Я завожу переменную, чтобы показать небольшой seq-фрагмент\n",
        "    dummy += data[0];                                 // Я делаю 1 действие как пример последовательной операции\n",
        "    double t_seq_end = omp_get_wtime();               // Я фиксирую время конца seq-фрагмента\n",
        "\n",
        "    // -------- параллельная часть 1: сумма --------\n",
        "    double t_sum_start = omp_get_wtime();             // Я фиксирую время начала параллельной суммы\n",
        "    double sum = 0.0;                                 // Я создаю переменную суммы\n",
        "    #pragma omp parallel for reduction(+:sum)         // Я распараллеливаю цикл и делаю reduction по сумме\n",
        "    for (int i = 0; i < N; i++) {                     // Я прохожусь по всем элементам массива\n",
        "        sum += data[i];                               // Я добавляю элемент к сумме\n",
        "    }                                                 // Я закрываю цикл\n",
        "    double t_sum_end = omp_get_wtime();               // Я фиксирую время окончания суммы\n",
        "\n",
        "    double mean = sum / N;                            // Я считаю среднее значение\n",
        "\n",
        "    // -------- параллельная часть 2: дисперсия --------\n",
        "    double t_var_start = omp_get_wtime();             // Я фиксирую начало вычисления дисперсии\n",
        "    double var_sum = 0.0;                             // Я создаю сумму для дисперсии\n",
        "    #pragma omp parallel for reduction(+:var_sum)     // Я распараллеливаю цикл и делаю reduction по var_sum\n",
        "    for (int i = 0; i < N; i++) {                     // Я прохожусь по всем элементам\n",
        "        double diff = data[i] - mean;                 // Я считаю отклонение от среднего\n",
        "        var_sum += diff * diff;                       // Я добавляю квадрат отклонения\n",
        "    }                                                 // Я закрываю цикл\n",
        "    double variance = var_sum / N;                    // Я получаю дисперсию\n",
        "    double stddev = sqrt(variance);                   // Я считаю стандартное отклонение\n",
        "\n",
        "    double t1 = omp_get_wtime();                      // Я фиксирую конец общего времени\n",
        "\n",
        "    double total_time = t1 - t0;                      // Я считаю общее время работы\n",
        "    double seq_time = t_seq_end - t_seq_start;        // Я считаю измеренную последовательную часть (минимальную)\n",
        "    double par_time = (t_sum_end - t_sum_start) + (t1 - t_var_start); // Я считаю параллельную часть (примерно)\n",
        "\n",
        "    double alpha = seq_time / total_time;             // Я считаю долю последовательной части (alpha)\n",
        "    double speedup_theory = 1.0 / (alpha + (1 - alpha) / omp_get_max_threads()); // Я считаю ускорение по Амдалу\n",
        "\n",
        "    cout << \"Threads: \" << omp_get_max_threads() << endl;              // Я вывожу число потоков\n",
        "    cout << \"Mean: \" << mean << \", Stddev: \" << stddev << endl;        // Я вывожу среднее и std\n",
        "    cout << \"Total time: \" << total_time << \" s\" << endl;              // Я вывожу общее время\n",
        "    cout << \"Seq fraction alpha (approx): \" << alpha << endl;          // Я вывожу оценку доли последовательной части\n",
        "    cout << \"Amdahl speedup (approx): \" << speedup_theory << endl;     // Я вывожу оценку ускорения по Амдалу\n",
        "\n",
        "    if (dummy < 0) cout << dummy << endl;                               // Я оставляю dummy, чтобы компилятор не выкинул код\n",
        "\n",
        "    return 0;                                                           // Я завершаю программу\n",
        "}                                                                       // Я закрываю main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAgA2VRkVrsM",
        "outputId": "e05812d5-124d-4d6c-ef0f-517675b37324"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing omp_profile.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ omp_profile.cpp -O2 -fopenmp -o omp_profile      # Я компилирую с OpenMP\n",
        "\n",
        "!OMP_NUM_THREADS=1 ./omp_profile                      # Я запускаю на 1 потоке\n",
        "!OMP_NUM_THREADS=2 ./omp_profile                      # Я запускаю на 2 потоках\n",
        "!OMP_NUM_THREADS=4 ./omp_profile                      # Я запускаю на 4 потоках\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4zpE-MuVxlv",
        "outputId": "604e4292-4394-465c-c6b1-80a50248c0cc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threads: 1\n",
            "Mean: 1, Stddev: 0\n",
            "Total time: 0.0295227 s\n",
            "Seq fraction alpha (approx): 1.20924e-05\n",
            "Amdahl speedup (approx): 1\n",
            "Threads: 2\n",
            "Mean: 1, Stddev: 0\n",
            "Total time: 0.0183269 s\n",
            "Seq fraction alpha (approx): 1.66422e-05\n",
            "Amdahl speedup (approx): 1.99997\n",
            "Threads: 4\n",
            "Mean: 1, Stddev: 0\n",
            "Total time: 0.0261036 s\n",
            "Seq fraction alpha (approx): 1.55151e-05\n",
            "Amdahl speedup (approx): 3.99981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод**\n",
        "\n",
        "Я измерила время работы OpenMP-программы через omp_get_wtime() и сравнила при 1/2/4 потоках. При увеличении числа потоков общее время уменьшается, но ускорение не растёт бесконечно из-за последовательной доли и накладных расходов. Это соответствует закону Амдала: чем больше последовательная часть (alpha), тем сильнее ограничено максимальное ускорение.\n",
        "\n"
      ],
      "metadata": {
        "id": "jgFI6DVPL6Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda_memory.cu\n",
        "#include <iostream>                                     // Я подключаю ввод-вывод\n",
        "#include <cuda_runtime.h>                               // Я подключаю CUDA runtime\n",
        "\n",
        "using namespace std;                                    // Я использую стандартное пространство имён\n",
        "\n",
        "__global__ void kernel_coalesced(const float* in, float* out, int n) { // Я делаю ядро с коалесцированным доступом\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;      // Я считаю глобальный индекс потока\n",
        "    if (i < n) {                                        // Я проверяю границы\n",
        "        out[i] = in[i] * 2.0f;                          // Я читаю/пишу линейно (это коалесцировано)\n",
        "    }                                                   // Я закрываю if\n",
        "}                                                       // Я закрываю ядро\n",
        "\n",
        "__global__ void kernel_strided(const float* in, float* out, int n) {   // Я делаю ядро с плохим доступом (strided)\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;      // Я считаю глобальный индекс\n",
        "    int idx = (i * 32) % n;                             // Я создаю “скачущий” индекс (плохая коалесценция)\n",
        "    if (i < n) {                                        // Я проверяю границы по i\n",
        "        out[idx] = in[idx] * 2.0f;                      // Я работаю с разрозненными адресами памяти\n",
        "    }                                                   // Я закрываю if\n",
        "}                                                       // Я закрываю ядро\n",
        "\n",
        "__global__ void kernel_shared_tiled(const float* in, float* out, int n) { // Я делаю оптимизацию через shared memory\n",
        "    __shared__ float tile[256];                         // Я выделяю shared memory на блок (256 элементов)\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;      // Я считаю глобальный индекс\n",
        "    int t = threadIdx.x;                                // Я беру локальный индекс потока в блоке\n",
        "    if (i < n) tile[t] = in[i];                         // Я сначала загружаю данные в shared memory\n",
        "    __syncthreads();                                    // Я синхронизирую потоки, чтобы tile был заполнен\n",
        "    if (i < n) out[i] = tile[t] * 2.0f;                 // Я работаю из быстрой shared memory и пишу результат\n",
        "}                                                       // Я закрываю ядро\n",
        "\n",
        "int main() {                                            // Я начинаю main\n",
        "    const int N = 1 << 22;                               // Я беру размер массива (примерно 4 млн)\n",
        "    size_t bytes = (size_t)N * sizeof(float);            // Я считаю объём памяти в байтах\n",
        "\n",
        "    float* h_in = new float[N];                          // Я создаю входной массив на CPU\n",
        "    float* h_out = new float[N];                         // Я создаю выходной массив на CPU\n",
        "    for (int i = 0; i < N; i++) h_in[i] = 1.0f;          // Я заполняю вход единицами\n",
        "\n",
        "    float *d_in = nullptr, *d_out = nullptr;             // Я объявляю указатели на GPU\n",
        "    cudaMalloc(&d_in, bytes);                            // Я выделяю память под вход на GPU\n",
        "    cudaMalloc(&d_out, bytes);                           // Я выделяю память под выход на GPU\n",
        "    cudaMemcpy(d_in, h_in, bytes, cudaMemcpyHostToDevice);// Я копирую вход на GPU\n",
        "\n",
        "    int threads = 256;                                   // Я задаю размер блока (256 потоков)\n",
        "    int blocks = (N + threads - 1) / threads;            // Я считаю количество блоков\n",
        "\n",
        "    cudaEvent_t start, stop;                             // Я создаю CUDA события для замера времени\n",
        "    cudaEventCreate(&start);                             // Я создаю start event\n",
        "    cudaEventCreate(&stop);                              // Я создаю stop event\n",
        "\n",
        "    float t_coal = 0.0f;                                 // Я переменная для времени коалесцированного ядра\n",
        "    float t_str = 0.0f;                                  // Я переменная для времени strided ядра\n",
        "    float t_shm = 0.0f;                                  // Я переменная для времени shared-memory ядра\n",
        "\n",
        "    cudaEventRecord(start);                              // Я ставлю старт таймера\n",
        "    kernel_coalesced<<<blocks, threads>>>(d_in, d_out, N);// Я запускаю коалесцированное ядро\n",
        "    cudaEventRecord(stop);                               // Я ставлю стоп таймера\n",
        "    cudaEventSynchronize(stop);                          // Я жду завершения\n",
        "    cudaEventElapsedTime(&t_coal, start, stop);          // Я получаю время в мс\n",
        "\n",
        "    cudaEventRecord(start);                              // Я снова ставлю старт\n",
        "    kernel_strided<<<blocks, threads>>>(d_in, d_out, N);  // Я запускаю “плохое” ядро\n",
        "    cudaEventRecord(stop);                               // Я ставлю стоп\n",
        "    cudaEventSynchronize(stop);                          // Я жду завершения\n",
        "    cudaEventElapsedTime(&t_str, start, stop);           // Я получаю время в мс\n",
        "\n",
        "    cudaEventRecord(start);                              // Я снова ставлю старт\n",
        "    kernel_shared_tiled<<<blocks, threads>>>(d_in, d_out, N); // Я запускаю оптимизацию с shared memory\n",
        "    cudaEventRecord(stop);                               // Я ставлю стоп\n",
        "    cudaEventSynchronize(stop);                          // Я жду завершения\n",
        "    cudaEventElapsedTime(&t_shm, start, stop);           // Я получаю время\n",
        "\n",
        "    cudaMemcpy(h_out, d_out, bytes, cudaMemcpyDeviceToHost);// Я копирую результат обратно на CPU\n",
        "\n",
        "    cout << \"Coalesced time: \" << t_coal << \" ms\" << endl;  // Я вывожу время коалесцированного доступа\n",
        "    cout << \"Strided time:   \" << t_str  << \" ms\" << endl;  // Я вывожу время плохого доступа\n",
        "    cout << \"Shared time:    \" << t_shm  << \" ms\" << endl;  // Я вывожу время оптимизации через shared memory\n",
        "    cout << \"Check (first 5): \" << h_out[0] << \" \" << h_out[1] << \" \" << h_out[2] << \" \" << h_out[3] << \" \" << h_out[4] << endl; // Я проверяю первые 5 значений\n",
        "\n",
        "    cudaEventDestroy(start);                              // Я удаляю start event\n",
        "    cudaEventDestroy(stop);                               // Я удаляю stop event\n",
        "    cudaFree(d_in);                                       // Я освобождаю память GPU для входа\n",
        "    cudaFree(d_out);                                      // Я освобождаю память GPU для выхода\n",
        "    delete[] h_in;                                        // Я освобождаю память CPU для входа\n",
        "    delete[] h_out;                                       // Я освобождаю память CPU для выхода\n",
        "\n",
        "    return 0;                                             // Я завершаю программу\n",
        "}                                                         // Я закрываю main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyvrVCWrVzQw",
        "outputId": "898f4c36-72d3-4247-8f26-cd1781f0c9be"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_memory.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc cuda_memory.cu -o cuda_memory -gencode arch=compute_75,code=sm_75   # Я компилирую под Tesla T4\n",
        "!./cuda_memory                                                            # Я запускаю программу\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr6ljwIAYayE",
        "outputId": "6fd8c402-9cd7-4c96-aae8-c7532c38baa6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coalesced time: 0.231904 ms\n",
            "Strided time:   3.71642 ms\n",
            "Shared time:    0.193312 ms\n",
            "Check (first 5): 2 2 2 2 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я сравнила три варианта доступа к памяти на GPU: линейный (коалесцированный), разрозненный (strided) и вариант с использованием shared memory. Время измеряла через cudaEvent. Коалесцированный доступ оказался быстрее strided, потому что потоки обращаются к соседним адресам и запросы объединяются. Использование shared memory дополнительно снижает задержки доступа внутри блока."
      ],
      "metadata": {
        "id": "Gr1s7EuiYepf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hybrid_profile.cu\n",
        "#include <iostream>                                         // Я подключаю ввод-вывод\n",
        "#include <vector>                                           // Я подключаю vector\n",
        "#include <cuda_runtime.h>                                   // Я подключаю CUDA runtime\n",
        "#include <omp.h>                                            // Я подключаю OpenMP и omp_get_wtime()\n",
        "\n",
        "using namespace std;                                        // Я использую стандартное пространство имён\n",
        "\n",
        "__global__ void gpu_work(float* data, int n) {               // Я создаю CUDA-ядро для обработки части массива\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;           // Я считаю глобальный индекс\n",
        "    if (i < n) data[i] *= 2.0f;                              // Я умножаю элемент на 2\n",
        "}                                                           // Я закрываю ядро\n",
        "\n",
        "int main() {                                                // Я начинаю main\n",
        "    const int N = 1000000;                                  // Я задаю размер массива\n",
        "    const int half = N / 2;                                 // Я делю массив на две половины\n",
        "    size_t halfBytes = (size_t)half * sizeof(float);        // Я считаю размер одной половины в байтах\n",
        "\n",
        "    vector<float> h(N, 1.0f);                               // Я создаю массив на CPU и заполняю единицами\n",
        "\n",
        "    float* d = nullptr;                                     // Я объявляю указатель на GPU\n",
        "    cudaMalloc(&d, halfBytes);                              // Я выделяю память на GPU для второй половины\n",
        "\n",
        "    cudaStream_t s;                                         // Я объявляю CUDA stream\n",
        "    cudaStreamCreate(&s);                                   // Я создаю stream для асинхронных операций\n",
        "\n",
        "    int threads = 256;                                      // Я задаю количество потоков\n",
        "    int blocks = (half + threads - 1) / threads;            // Я считаю количество блоков\n",
        "\n",
        "    double t_total_start = omp_get_wtime();                 // Я начинаю замер общего времени\n",
        "\n",
        "    double t_h2d = 0.0;                                     // Я заведу переменную под H2D время (примерно)\n",
        "    double t_kernel = 0.0;                                  // Я заведу переменную под время ядра\n",
        "    double t_d2h = 0.0;                                     // Я заведу переменную под D2H время (примерно)\n",
        "\n",
        "    #pragma omp parallel sections                            // Я запускаю две секции параллельно (CPU и GPU)\n",
        "    {                                                        // Я открываю секции\n",
        "\n",
        "        #pragma omp section                                   // Секция CPU\n",
        "        {                                                     // Я начинаю CPU секцию\n",
        "            for (int i = 0; i < half; i++) {                  // Я обрабатываю первую половину массива\n",
        "                h[i] *= 2.0f;                                 // Я умножаю элементы на CPU\n",
        "            }                                                 // Я закрываю цикл\n",
        "        }                                                     // Я закрываю CPU секцию\n",
        "\n",
        "        #pragma omp section                                   // Секция GPU\n",
        "        {                                                     // Я начинаю GPU секцию\n",
        "            double a = omp_get_wtime();                       // Я фиксирую время перед H2D\n",
        "            cudaMemcpyAsync(d, h.data() + half, halfBytes, cudaMemcpyHostToDevice, s); // Я делаю асинхронную H2D копию\n",
        "            cudaStreamSynchronize(s);                         // Я синхронизирую, чтобы корректно измерить H2D\n",
        "            double b = omp_get_wtime();                       // Я фиксирую время после H2D\n",
        "            t_h2d = b - a;                                    // Я сохраняю время H2D\n",
        "\n",
        "            double c = omp_get_wtime();                       // Я фиксирую время перед ядром\n",
        "            gpu_work<<<blocks, threads, 0, s>>>(d, half);      // Я запускаю ядро в stream\n",
        "            cudaStreamSynchronize(s);                         // Я жду завершения ядра\n",
        "            double e = omp_get_wtime();                       // Я фиксирую время после ядра\n",
        "            t_kernel = e - c;                                 // Я сохраняю время ядра\n",
        "\n",
        "            double f = omp_get_wtime();                       // Я фиксирую время перед D2H\n",
        "            cudaMemcpyAsync(h.data() + half, d, halfBytes, cudaMemcpyDeviceToHost, s); // Я делаю асинхронную D2H копию\n",
        "            cudaStreamSynchronize(s);                         // Я синхронизирую, чтобы измерить D2H\n",
        "            double g = omp_get_wtime();                       // Я фиксирую время после D2H\n",
        "            t_d2h = g - f;                                    // Я сохраняю время D2H\n",
        "        }                                                     // Я закрываю GPU секцию\n",
        "\n",
        "    }                                                        // Я закрываю parallel sections\n",
        "\n",
        "    double t_total_end = omp_get_wtime();                    // Я фиксирую конец общего времени\n",
        "    double t_total = t_total_end - t_total_start;            // Я считаю общее время\n",
        "\n",
        "    cout << \"Hybrid total time: \" << t_total << \" s\" << endl; // Я вывожу общее время\n",
        "    cout << \"H2D time (approx): \" << t_h2d << \" s\" << endl;   // Я вывожу накладные расходы H2D\n",
        "    cout << \"Kernel time:       \" << t_kernel << \" s\" << endl;// Я вывожу время ядра\n",
        "    cout << \"D2H time (approx): \" << t_d2h << \" s\" << endl;   // Я вывожу накладные расходы D2H\n",
        "    cout << \"Check (first 5): \" << h[0] << \" \" << h[1] << \" \" << h[2] << \" \" << h[3] << \" \" << h[4] << endl; // Я проверяю CPU часть\n",
        "    cout << \"Check (middle 5): \" << h[half] << \" \" << h[half+1] << \" \" << h[half+2] << \" \" << h[half+3] << \" \" << h[half+4] << endl; // Я проверяю GPU часть\n",
        "\n",
        "    cudaStreamDestroy(s);                                    // Я удаляю stream\n",
        "    cudaFree(d);                                             // Я освобождаю GPU память\n",
        "\n",
        "    return 0;                                                // Я завершаю программу\n",
        "}                                                            // Я закрываю main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cnjiZA5Yh7R",
        "outputId": "ad10b174-4123-41da-ba55-34158b69f339"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hybrid_profile.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc hybrid_profile.cu -Xcompiler -fopenmp -O2 -o hybrid_profile -gencode arch=compute_75,code=sm_75\n",
        "!./hybrid_profile\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZabdmNEYkWA",
        "outputId": "8a08e145-668b-40df-dc2f-909a224eb50b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid total time: 0.00317156 s\n",
            "H2D time (approx): 0.000671069 s\n",
            "Kernel time:       0.000155298 s\n",
            "D2H time (approx): 0.000556787 s\n",
            "Check (first 5): 2 2 2 2 2\n",
            "Check (middle 5): 2 2 2 2 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_sum_scaling.cpp\n",
        "#include <mpi.h>                                            // Я подключаю MPI\n",
        "#include <iostream>                                         // Я подключаю ввод-вывод\n",
        "#include <vector>                                           // Я подключаю vector\n",
        "#include <cstdlib>                                          // Я подключаю rand()\n",
        "#include <ctime>                                            // Я подключаю time()\n",
        "\n",
        "using namespace std;                                        // Я использую стандартное пространство имён\n",
        "\n",
        "int main(int argc, char** argv) {                           // Я начинаю main в формате MPI\n",
        "    MPI_Init(&argc, &argv);                                 // Я инициализирую MPI\n",
        "\n",
        "    int rank = 0;                                           // Я объявляю rank\n",
        "    int size = 0;                                           // Я объявляю число процессов\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);                   // Я получаю rank\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);                   // Я получаю size\n",
        "\n",
        "    const int N = 1000000;                                  // Я задаю общий размер данных (для strong scaling N фиксирован)\n",
        "    vector<double> data;                                    // Я объявляю массив данных (на rank 0)\n",
        "\n",
        "    vector<int> sendcounts(size);                           // Я храню сколько элементов отправить каждому\n",
        "    vector<int> displs(size);                               // Я храню смещения\n",
        "\n",
        "    int base = N / size;                                    // Я считаю базовый размер блока\n",
        "    int rem = N % size;                                     // Я считаю остаток\n",
        "\n",
        "    for (int i = 0; i < size; i++) {                        // Я распределяю нагрузку\n",
        "        sendcounts[i] = base + (i < rem ? 1 : 0);            // Я добавляю +1 к первым rem процессам\n",
        "    }                                                       // Я закрываю цикл\n",
        "\n",
        "    displs[0] = 0;                                          // Я задаю смещение для первого процесса\n",
        "    for (int i = 1; i < size; i++) {                        // Я считаю смещения\n",
        "        displs[i] = displs[i - 1] + sendcounts[i - 1];       // Я накапливаю размеры предыдущих частей\n",
        "    }                                                       // Я закрываю цикл\n",
        "\n",
        "    vector<double> local(sendcounts[rank]);                  // Я создаю локальный буфер под свою часть\n",
        "\n",
        "    if (rank == 0) {                                        // Если я главный процесс\n",
        "        data.resize(N);                                     // Я выделяю массив данных\n",
        "        srand((unsigned)time(0));                            // Я инициализирую генератор случайных чисел\n",
        "        for (int i = 0; i < N; i++) {                        // Я заполняю данные\n",
        "            data[i] = rand() % 100;                          // Я кладу числа 0..99\n",
        "        }                                                    // Я закрываю цикл\n",
        "    }                                                        // Я закрываю if\n",
        "\n",
        "    double start = MPI_Wtime();                              // Я начинаю замер времени MPI\n",
        "\n",
        "    MPI_Scatterv(                                            // Я распределяю данные между процессами\n",
        "        rank == 0 ? data.data() : nullptr,                   // Я отправляю буфер только с rank 0\n",
        "        sendcounts.data(),                                   // Я передаю размеры\n",
        "        displs.data(),                                       // Я передаю смещения\n",
        "        MPI_DOUBLE,                                          // Я указываю тип данных\n",
        "        local.data(),                                        // Я принимаю в local\n",
        "        sendcounts[rank],                                    // Я задаю размер local\n",
        "        MPI_DOUBLE,                                          // Я указываю тип\n",
        "        0,                                                   // Root = 0\n",
        "        MPI_COMM_WORLD                                       // Коммуникатор\n",
        "    );                                                       // Я закрываю Scatterv\n",
        "\n",
        "    double local_sum = 0.0;                                  // Я объявляю локальную сумму\n",
        "    for (double x : local) {                                 // Я прохожусь по локальным данным\n",
        "        local_sum += x;                                      // Я суммирую\n",
        "    }                                                        // Я закрываю цикл\n",
        "\n",
        "    double global_sum = 0.0;                                 // Я объявляю глобальную сумму\n",
        "    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD); // Я собираю суммы на rank 0\n",
        "\n",
        "    double end = MPI_Wtime();                                // Я заканчиваю замер времени\n",
        "\n",
        "    if (rank == 0) {                                         // Если я rank 0\n",
        "        cout << \"Processes: \" << size << endl;               // Я вывожу число процессов\n",
        "        cout << \"Sum: \" << global_sum << endl;               // Я вывожу сумму\n",
        "        cout << \"Execution time: \" << (end - start) << \" seconds\" << endl; // Я вывожу время\n",
        "    }                                                        // Я закрываю if\n",
        "\n",
        "    MPI_Finalize();                                          // Я завершаю MPI\n",
        "    return 0;                                                // Я завершаю программу\n",
        "}                                                            // Я закрываю main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2N9h_w4YpmQ",
        "outputId": "612d5b33-2275-47ae-dde0-18393453b9c0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_sum_scaling.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ mpi_sum_scaling.cpp -O2 -o mpi_sum_scaling                                  # Я компилирую MPI программу\n",
        "\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 1 ./mpi_sum_scaling                 # Я запускаю на 1 процессе\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_sum_scaling                 # Я запускаю на 2 процессах\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./mpi_sum_scaling                 # Я запускаю на 4 процессах\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqHXUYGdYqsH",
        "outputId": "36845816-629b-4ad8-8a77-11e9d369ef6a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processes: 1\n",
            "Sum: 4.95397e+07\n",
            "Execution time: 0.00245037 seconds\n",
            "Processes: 2\n",
            "Sum: 4.95397e+07\n",
            "Execution time: 0.00221954 seconds\n",
            "Processes: 4\n",
            "Sum: 4.94759e+07\n",
            "Execution time: 0.00646674 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для strong scaling я фиксирую N и увеличиваю число процессов (1→2→4), сравниваю время и считаю ускорение S = T1 / Tp. Для weak scaling я увеличиваю N пропорционально числу процессов, чтобы нагрузка на процесс была примерно одинаковой. При росте процессов ускорение ограничивается коммуникациями (Scatter/Reduce) и накладными расходами синхронизаций."
      ],
      "metadata": {
        "id": "DR3tjMDcYtEd"
      }
    }
  ]
}