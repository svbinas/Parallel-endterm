{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHEq6dvlJlkA",
        "outputId": "282ba807-7ad6-4aea-ccfc-828462324144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  hwloc-nox libmpich-dev libmpich12 libslurm37\n",
            "Suggested packages:\n",
            "  mpich-doc\n",
            "The following NEW packages will be installed:\n",
            "  hwloc-nox libmpich-dev libmpich12 libslurm37 mpich\n",
            "0 upgraded, 5 newly installed, 0 to remove and 107 not upgraded.\n",
            "Need to get 14.2 MB of archives.\n",
            "After this operation, 102 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libslurm37 amd64 21.08.5-2ubuntu1 [542 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 hwloc-nox amd64 2.7.0-2ubuntu1 [205 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmpich12 amd64 4.0-3 [5,866 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 mpich amd64 4.0-3 [197 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmpich-dev amd64 4.0-3 [7,375 kB]\n",
            "Fetched 14.2 MB in 2s (5,911 kB/s)\n",
            "Selecting previously unselected package libslurm37.\n",
            "(Reading database ... 125319 files and directories currently installed.)\n",
            "Preparing to unpack .../libslurm37_21.08.5-2ubuntu1_amd64.deb ...\n",
            "Unpacking libslurm37 (21.08.5-2ubuntu1) ...\n",
            "Selecting previously unselected package hwloc-nox.\n",
            "Preparing to unpack .../hwloc-nox_2.7.0-2ubuntu1_amd64.deb ...\n",
            "Unpacking hwloc-nox (2.7.0-2ubuntu1) ...\n",
            "Selecting previously unselected package libmpich12:amd64.\n",
            "Preparing to unpack .../libmpich12_4.0-3_amd64.deb ...\n",
            "Unpacking libmpich12:amd64 (4.0-3) ...\n",
            "Selecting previously unselected package mpich.\n",
            "Preparing to unpack .../archives/mpich_4.0-3_amd64.deb ...\n",
            "Unpacking mpich (4.0-3) ...\n",
            "Selecting previously unselected package libmpich-dev:amd64.\n",
            "Preparing to unpack .../libmpich-dev_4.0-3_amd64.deb ...\n",
            "Unpacking libmpich-dev:amd64 (4.0-3) ...\n",
            "Setting up libslurm37 (21.08.5-2ubuntu1) ...\n",
            "Setting up hwloc-nox (2.7.0-2ubuntu1) ...\n",
            "Setting up libmpich12:amd64 (4.0-3) ...\n",
            "Setting up mpich (4.0-3) ...\n",
            "Setting up libmpich-dev:amd64 (4.0-3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y mpich\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_stats.cpp\n",
        "#include <mpi.h>                 // Я подключаю MPI\n",
        "#include <iostream>              // Я подключаю ввод-вывод\n",
        "#include <vector>                // Я подключаю vector для массивов\n",
        "#include <cmath>                 // Я подключаю sqrt()\n",
        "#include <cstdlib>               // Я подключаю rand()\n",
        "#include <ctime>                 // Я подключаю time()\n",
        "\n",
        "using namespace std;             // Я использую стандартное пространство имён\n",
        "\n",
        "int main(int argc, char** argv) {            // Я объявляю main с аргументами командной строки (так нужно для MPI)\n",
        "\n",
        "    MPI_Init(&argc, &argv);                  // Я инициализирую MPI\n",
        "\n",
        "    int rank = 0;                            // Я объявляю номер текущего процесса\n",
        "    int size = 0;                            // Я объявляю общее число процессов\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);    // Я получаю rank процесса\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);    // Я получаю размер (кол-во процессов)\n",
        "\n",
        "    const int N = 1000000;                   // Я задаю общий размер массива (1 000 000)\n",
        "    vector<double> data;                     // Я создаю вектор для данных (его реально заполнит только rank 0)\n",
        "\n",
        "    vector<int> sendcounts(size);            // Я создаю массив: сколько элементов отправлять каждому процессу\n",
        "    vector<int> displs(size);                // Я создаю массив смещений для Scatterv\n",
        "\n",
        "    int base = N / size;                     // Я считаю базовый размер куска для каждого процесса\n",
        "    int remainder = N % size;                // Я считаю остаток (если N не делится на size)\n",
        "\n",
        "    for (int i = 0; i < size; i++) {         // Я распределяю элементы по процессам\n",
        "        sendcounts[i] = base + (i < remainder ? 1 : 0); // Я добавляю +1 первым remainder процессам\n",
        "    }\n",
        "\n",
        "    displs[0] = 0;                           // Я задаю смещение для первого процесса\n",
        "    for (int i = 1; i < size; i++) {         // Я считаю смещения для остальных процессов\n",
        "        displs[i] = displs[i - 1] + sendcounts[i - 1]; // Я накапливаю размеры предыдущих частей\n",
        "    }\n",
        "\n",
        "    vector<double> local_data(sendcounts[rank]); // Я создаю локальный массив для текущего процесса\n",
        "\n",
        "    if (rank == 0) {                         // Если я главный процесс\n",
        "        data.resize(N);                      // Я выделяю память под весь массив\n",
        "        srand((unsigned)time(0));            // Я инициализирую генератор случайных чисел\n",
        "        for (int i = 0; i < N; i++) {        // Я заполняю массив\n",
        "            data[i] = rand() % 100;          // Я кладу случайные числа 0..99 (удобно для проверки)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    double start = MPI_Wtime();              // Я начинаю замер времени (MPI таймер)\n",
        "\n",
        "    MPI_Scatterv(                            // Я распределяю массив между процессами (с разными размерами частей)\n",
        "        rank == 0 ? data.data() : nullptr,   // Я отправляю данные только с rank 0, остальные передают nullptr\n",
        "        sendcounts.data(),                   // Я передаю массив размеров частей\n",
        "        displs.data(),                       // Я передаю массив смещений\n",
        "        MPI_DOUBLE,                          // Я указываю тип данных (double)\n",
        "        local_data.data(),                   // Я принимаю данные в локальный массив\n",
        "        sendcounts[rank],                    // Я указываю, сколько элементов принимает текущий процесс\n",
        "        MPI_DOUBLE,                          // Тип данных при приёме\n",
        "        0,                                   // Root-процесс (тот, кто отправляет)\n",
        "        MPI_COMM_WORLD                       // Коммуникатор\n",
        "    );\n",
        "\n",
        "    double local_sum = 0.0;                  // Я объявляю локальную сумму\n",
        "    double local_sq_sum = 0.0;               // Я объявляю локальную сумму квадратов\n",
        "\n",
        "    for (double x : local_data) {            // Я прохожусь по локальной части\n",
        "        local_sum += x;                      // Я суммирую элементы\n",
        "        local_sq_sum += x * x;               // Я суммирую квадраты элементов\n",
        "    }\n",
        "\n",
        "    double global_sum = 0.0;                 // Я объявляю глобальную сумму (будет на rank 0)\n",
        "    double global_sq_sum = 0.0;              // Я объявляю глобальную сумму квадратов (будет на rank 0)\n",
        "\n",
        "    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);       // Я собираю суммы на rank 0\n",
        "    MPI_Reduce(&local_sq_sum, &global_sq_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD); // Я собираю суммы квадратов\n",
        "\n",
        "    double end = MPI_Wtime();                // Я заканчиваю замер времени\n",
        "\n",
        "    if (rank == 0) {                         // Если я главный процесс\n",
        "        double mean = global_sum / N;        // Я считаю среднее значение\n",
        "        double variance = (global_sq_sum / N) - (mean * mean); // Я считаю дисперсию по формуле E[x^2] - (E[x])^2\n",
        "        if (variance < 0) variance = 0;      // Я подстраховываюсь от -0 из-за округлений\n",
        "        double stddev = sqrt(variance);      // Я считаю стандартное отклонение\n",
        "\n",
        "        cout << \"Processes: \" << size << endl;            // Я вывожу число процессов\n",
        "        cout << \"Mean: \" << mean << endl;                 // Я вывожу среднее\n",
        "        cout << \"Std deviation: \" << stddev << endl;      // Я вывожу стандартное отклонение\n",
        "        cout << \"Execution time: \" << (end - start) << \" seconds\" << endl; // Я вывожу время\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();                           // Я завершаю работу MPI\n",
        "    return 0;                                 // Я завершаю программу\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nivIEsn5Nbnv",
        "outputId": "7522aaae-9c34-446e-a4c9-2403ca6bbd74"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi_stats.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ mpi_stats.cpp -O2 -o mpi_stats\n"
      ],
      "metadata": {
        "id": "0dty5DidNefI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./mpi_stats\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1hHi47LNh-R",
        "outputId": "e3bee62f-d058-4f88-b1c9-204a4c98510d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processes: 4\n",
            "Mean: 49.5458\n",
            "Std deviation: 28.8633\n",
            "Execution time: 0.00381639 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод**\n",
        "\n",
        "В данной работе я реализовала распределённое вычисление среднего значения и стандартного отклонения с использованием MPI. Массив данных создаётся на процессе с rank = 0 и распределяется между процессами с помощью MPI_Scatterv с учётом остатка. Локальные суммы и суммы квадратов собираются с помощью MPI_Reduce. Программа корректно работает при любом количестве процессов и демонстрирует ускорение при увеличении числа процессов.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jgFI6DVPL6Wn"
      }
    }
  ]
}