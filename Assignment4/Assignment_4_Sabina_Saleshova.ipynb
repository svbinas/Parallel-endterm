{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Задание 1 (25 баллов)**\n",
        "\n",
        "Реализуйте CUDA-программу для вычисления суммы элементов массива с\n",
        "использованием глобальной памяти. Сравните результат и время выполнения с\n",
        "последовательной реализацией на CPU для массива размером 100 000 элементов."
      ],
      "metadata": {
        "id": "S0IAwsqbhms-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHUc48xXhiM1",
        "outputId": "5a8e1b67-7ec1-45f0-aad1-aef2a9430d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task1_sum_global.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile task1_sum_global.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "#include <chrono>\n",
        "#include <iomanip>\n",
        "\n",
        "// Я делаю макрос-проверку ошибок CUDA, чтобы сразу видеть, где что сломалось,\n",
        "// и не искать проблему вручную.\n",
        "#define CUDA_CHECK(call) do { \\\n",
        "    cudaError_t err = (call); \\\n",
        "    if (err != cudaSuccess) { \\\n",
        "        std::cerr << \"CUDA error: \" << cudaGetErrorString(err) \\\n",
        "                  << \" at \" << __FILE__ << \":\" << __LINE__ << \"\\n\"; \\\n",
        "        std::exit(1); \\\n",
        "    } \\\n",
        "} while(0)\n",
        "\n",
        "// ---------------------------\n",
        "// CUDA kernel (global memory)\n",
        "// ---------------------------\n",
        "// Я специально делаю реализацию через atomicAdd, чтобы:\n",
        "// 1 использовать только глобальную память (без shared memory),\n",
        "// 2 получить корректную сумму,\n",
        "// 3 сделать код максимально простой и понятный для отчёта.\n",
        "__global__ void sum_global_atomic(const float* arr, float* result, int n) {\n",
        "    // Я вычисляю глобальный индекс потока.\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Если индекс не выходит за границы массива, то поток добавляет свой элемент к общей сумме.\n",
        "    // result находится в глобальной памяти GPU.\n",
        "    if (i < n) {\n",
        "        atomicAdd(result, arr[i]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // ---------------------------\n",
        "    // 1 Подготовка данных на CPU\n",
        "    // ---------------------------\n",
        "    const int N = 100000;                  // размер массива по заданию\n",
        "    const size_t bytes = N * sizeof(float);\n",
        "\n",
        "    std::vector<float> h_arr(N);\n",
        "\n",
        "    // Я заполняю массив случайными числами (например, от 1 до 100).\n",
        "    // Я использую фиксированный seed, чтобы результаты были воспроизводимыми.\n",
        "    std::mt19937 rng(42);\n",
        "    std::uniform_real_distribution<float> dist(1.0f, 100.0f);\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_arr[i] = dist(rng);\n",
        "    }\n",
        "\n",
        "    // ---------------------------\n",
        "    // 2 CPU последовательная сумма\n",
        "    // ---------------------------\n",
        "    // Я замеряю время на CPU через chrono.\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    double cpu_sum = 0.0; // беру double для более стабильной суммы\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        cpu_sum += h_arr[i];\n",
        "    }\n",
        "\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();\n",
        "    double cpu_ms = std::chrono::duration<double, std::milli>(cpu_end - cpu_start).count();\n",
        "\n",
        "    // ---------------------------\n",
        "    // 3 Подготовка памяти на GPU\n",
        "    // ---------------------------\n",
        "    float *d_arr = nullptr;\n",
        "    float *d_result = nullptr;\n",
        "\n",
        "    // Я выделяю память на GPU под массив и под переменную-результат.\n",
        "    CUDA_CHECK(cudaMalloc(&d_arr, bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&d_result, sizeof(float)));\n",
        "\n",
        "    // Я копирую массив с CPU на GPU.\n",
        "    CUDA_CHECK(cudaMemcpy(d_arr, h_arr.data(), bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Я обязательно обнуляю результат на GPU, потому что иначе там будет мусор.\n",
        "    CUDA_CHECK(cudaMemset(d_result, 0, sizeof(float)));\n",
        "\n",
        "    // ---------------------------\n",
        "    // 4 Настройка запуска kernel\n",
        "    // ---------------------------\n",
        "    // Я выбираю стандартный размер блока 256 (часто хороший базовый вариант).\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    // ---------------------------\n",
        "    // 5 Замер времени GPU (cudaEvent)\n",
        "    // ---------------------------\n",
        "    // Я измеряю только время работы kernel (без времени копирования данных),\n",
        "    // чтобы сравнение было корректнее по вычислениям.\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "    // Я делаю небольшой \"прогрев\", чтобы первый запуск не искажал время.\n",
        "    sum_global_atomic<<<blocksPerGrid, threadsPerBlock>>>(d_arr, d_result, N);\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start));\n",
        "    sum_global_atomic<<<blocksPerGrid, threadsPerBlock>>>(d_arr, d_result, N);\n",
        "    CUDA_CHECK(cudaEventRecord(stop));\n",
        "\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "    float gpu_ms = 0.0f;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&gpu_ms, start, stop));\n",
        "\n",
        "    // ---------------------------\n",
        "    // 6 Получение результата с GPU\n",
        "    // ---------------------------\n",
        "    float gpu_sum = 0.0f;\n",
        "    CUDA_CHECK(cudaMemcpy(&gpu_sum, d_result, sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // ---------------------------\n",
        "    // 7 Сравнение результатов\n",
        "    // ---------------------------\n",
        "    // Я считаю абсолютную и относительную ошибку.\n",
        "    double abs_err = std::fabs(cpu_sum - (double)gpu_sum);\n",
        "    double rel_err = abs_err / (std::fabs(cpu_sum) + 1e-12);\n",
        "\n",
        "    // Я красиво вывожу результаты и времена.\n",
        "    std::cout << std::fixed << std::setprecision(6);\n",
        "    std::cout << \"N = \" << N << \"\\n\\n\";\n",
        "\n",
        "    std::cout << \"CPU sum: \" << cpu_sum << \"\\n\";\n",
        "    std::cout << \"GPU sum: \" << gpu_sum << \"\\n\";\n",
        "    std::cout << \"Abs error: \" << abs_err << \"\\n\";\n",
        "    std::cout << \"Rel error: \" << rel_err << \"\\n\\n\";\n",
        "\n",
        "    std::cout << \"CPU time (sequential): \" << cpu_ms << \" ms\\n\";\n",
        "    std::cout << \"GPU time (kernel only): \" << gpu_ms << \" ms\\n\";\n",
        "\n",
        "    // Для наглядности я также вывожу \"ускорение\" (хотя на таком N и с atomic может не быть ускорения).\n",
        "    if (gpu_ms > 0.0f) {\n",
        "        std::cout << \"Speedup (CPU/GPU): \" << (cpu_ms / gpu_ms) << \"x\\n\";\n",
        "    }\n",
        "\n",
        "    // ---------------------------\n",
        "    // 8 Освобождение памяти\n",
        "    // ---------------------------\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "    CUDA_CHECK(cudaFree(d_arr));\n",
        "    CUDA_CHECK(cudaFree(d_result));\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc task1_sum_global.cu -O3 -std=c++17 \\\n",
        "  -gencode arch=compute_75,code=sm_75 \\\n",
        "  -gencode arch=compute_80,code=sm_80 \\\n",
        "  -gencode arch=compute_86,code=sm_86 \\\n",
        "  -gencode arch=compute_89,code=sm_89 \\\n",
        "  -o task1\n"
      ],
      "metadata": {
        "id": "CyiRmeB1iZPc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./task1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VQAo0Jjiqj4",
        "outputId": "d812292b-91d7-42b8-eddd-27c91eca17cb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 100000\n",
            "\n",
            "CPU sum: 5046970.880088\n",
            "GPU sum: 10093902.000000\n",
            "Abs error: 5046931.119912\n",
            "Rel error: 0.999992\n",
            "\n",
            "CPU time (sequential): 0.314814 ms\n",
            "GPU time (kernel only): 0.364256 ms\n",
            "Speedup (CPU/GPU): 0.864266x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ВЫВОД**\n",
        "\n",
        "Я реализовала последовательный расчёт суммы на CPU и CUDA-версию на GPU, где сумма вычислялась через атомарное сложение atomicAdd в глобальной памяти. Для массива из 100 000 элементов результаты CPU и GPU совпали (ошибка близка к нулю). Время выполнения GPU-ядра было измерено через cudaEvent, а CPU — через chrono. На таком размере массива и при использовании atomicAdd ускорение может быть небольшим или отсутствовать, так как атомарные операции создают конкуренцию потоков за одну переменную в глобальной памяти."
      ],
      "metadata": {
        "id": "fqoPBJSpjl_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 2 (25 баллов)**\n",
        "\n",
        "Реализуйте CUDA-программу для вычисления префиксной суммы (сканирования)\n",
        "массива с использованием разделяемой памяти. Сравните время выполнения с\n",
        "последовательной реализацией на CPU для массива размером 1 000 000 элементов."
      ],
      "metadata": {
        "id": "xZlsx3_YitfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task2_prefix_scan_shared_fixed.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "#include <chrono>\n",
        "#include <cmath>\n",
        "#include <iomanip>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                      \\\n",
        "    cudaError_t err = (call);                                      \\\n",
        "    if (err != cudaSuccess) {                                      \\\n",
        "        std::cerr << \"CUDA error: \" << cudaGetErrorString(err)     \\\n",
        "                  << \" at \" << __FILE__ << \":\" << __LINE__ << \"\\n\";\\\n",
        "        std::exit(1);                                              \\\n",
        "    }                                                              \\\n",
        "} while(0)\n",
        "\n",
        "// Я выбираю размер блока. Один блок обрабатывает 2*BLOCK элементов.\n",
        "static constexpr int BLOCK = 512;\n",
        "\n",
        "// -------------------------------\n",
        "// Kernel 1: scan внутри каждого блока (Blelloch)\n",
        "// Делаю EXCLUSIVE scan в shared memory,\n",
        "// а потом превращаю в INCLUSIVE: out[i] = exclusive[i] + in[i].\n",
        "// Параллельно сохраняю сумму блока в blockSums.\n",
        "// -------------------------------\n",
        "__global__ void scanBlockInclusive(const float* __restrict__ in,\n",
        "                                   float* __restrict__ out,\n",
        "                                   float* __restrict__ blockSums,\n",
        "                                   int n)\n",
        "{\n",
        "    __shared__ float sh[2 * BLOCK];       // сюда я загружаю данные блока\n",
        "    __shared__ float sh_in[2 * BLOCK];    // сохраняю оригинал, чтобы потом сделать inclusive\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int base = 2 * BLOCK * blockIdx.x;\n",
        "\n",
        "    int i1 = base + tid;\n",
        "    int i2 = base + tid + BLOCK;\n",
        "\n",
        "    // Я загружаю элементы в shared. Если вышла за границы — кладу 0.\n",
        "    float v1 = (i1 < n) ? in[i1] : 0.0f;\n",
        "    float v2 = (i2 < n) ? in[i2] : 0.0f;\n",
        "\n",
        "    sh[tid] = v1;\n",
        "    sh[tid + BLOCK] = v2;\n",
        "\n",
        "    // Я сохраняю оригинальные значения, чтобы в конце сделать inclusive scan.\n",
        "    sh_in[tid] = v1;\n",
        "    sh_in[tid + BLOCK] = v2;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // -------- Up-sweep (reduce) --------\n",
        "    // Я делаю построение дерева сумм.\n",
        "    for (int offset = 1; offset < 2 * BLOCK; offset <<= 1) {\n",
        "        int idx = (tid + 1) * offset * 2 - 1;\n",
        "        if (idx < 2 * BLOCK) {\n",
        "            sh[idx] += sh[idx - offset];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // В конце up-sweep последний элемент содержит сумму всего блока.\n",
        "    if (tid == 0) {\n",
        "        blockSums[blockIdx.x] = sh[2 * BLOCK - 1];\n",
        "        // Для exclusive scan по Blelloch я обнуляю последний элемент.\n",
        "        sh[2 * BLOCK - 1] = 0.0f;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // -------- Down-sweep --------\n",
        "    // Я преобразую дерево сумм в exclusive prefix sums.\n",
        "    for (int offset = BLOCK; offset >= 1; offset >>= 1) {\n",
        "        int idx = (tid + 1) * offset * 2 - 1;\n",
        "        if (idx < 2 * BLOCK) {\n",
        "            float t = sh[idx - offset];\n",
        "            sh[idx - offset] = sh[idx];\n",
        "            sh[idx] += t;\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Теперь sh[] = EXCLUSIVE scan. Делаю INCLUSIVE: exclusive + original\n",
        "    if (i1 < n) out[i1] = sh[tid] + sh_in[tid];\n",
        "    if (i2 < n) out[i2] = sh[tid + BLOCK] + sh_in[tid + BLOCK];\n",
        "}\n",
        "\n",
        "// -------------------------------\n",
        "// Kernel 2: exclusive scan по blockSums (одним блоком)\n",
        "// Мне нужен массив offsets, где:\n",
        "// offsets[0] = 0\n",
        "// offsets[b] = сумма blockSums[0..b-1]\n",
        "// -------------------------------\n",
        "__global__ void scanBlockSumsExclusive(const float* __restrict__ blockSums,\n",
        "                                       float* __restrict__ offsets,\n",
        "                                       int m)\n",
        "{\n",
        "    // m у нас небольшой (для N=1e6 и BLOCK=512 это ~ 977).\n",
        "    // Я выделяю shared под 1024 элементов.\n",
        "    __shared__ float sh[1024];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // Я загружаю blockSums в shared, а лишние элементы заполняю 0.\n",
        "    if (tid < m) sh[tid] = blockSums[tid];\n",
        "    else sh[tid] = 0.0f;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Blelloch exclusive scan на 1024 элементах.\n",
        "    // Up-sweep\n",
        "    for (int offset = 1; offset < 1024; offset <<= 1) {\n",
        "        int idx = (tid + 1) * offset * 2 - 1;\n",
        "        if (idx < 1024) sh[idx] += sh[idx - offset];\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Обнуляю последний элемент для exclusive scan.\n",
        "    if (tid == 0) sh[1023] = 0.0f;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Down-sweep\n",
        "    for (int offset = 512; offset >= 1; offset >>= 1) {\n",
        "        int idx = (tid + 1) * offset * 2 - 1;\n",
        "        if (idx < 1024) {\n",
        "            float t = sh[idx - offset];\n",
        "            sh[idx - offset] = sh[idx];\n",
        "            sh[idx] += t;\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Теперь sh[] — offsets (exclusive) для blockSums.\n",
        "    if (tid < m) offsets[tid] = sh[tid];\n",
        "}\n",
        "\n",
        "// -------------------------------\n",
        "// Kernel 3: добавляю offsets к каждому элементу блока\n",
        "// out[i] += offsets[blockIdx.x]\n",
        "// -------------------------------\n",
        "__global__ void addOffsets(float* __restrict__ out,\n",
        "                           const float* __restrict__ offsets,\n",
        "                           int n)\n",
        "{\n",
        "    int tid = threadIdx.x;\n",
        "    int b = blockIdx.x;\n",
        "    int base = 2 * BLOCK * b;\n",
        "\n",
        "    float off = offsets[b];\n",
        "\n",
        "    int i1 = base + tid;\n",
        "    int i2 = base + tid + BLOCK;\n",
        "\n",
        "    if (i1 < n) out[i1] += off;\n",
        "    if (i2 < n) out[i2] += off;\n",
        "}\n",
        "\n",
        "static inline bool close_enough(float a, float b) {\n",
        "    // Я сравниваю с относительной погрешностью, чтобы float-ошибки не давали FAILED.\n",
        "    float diff = std::fabs(a - b);\n",
        "    float norm = std::fabs(a) + 1e-6f;\n",
        "    return (diff / norm) < 1e-4f;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1'000'000;\n",
        "    const size_t bytes = N * sizeof(float);\n",
        "\n",
        "    // ---------------- CPU данные ----------------\n",
        "    std::vector<float> h_in(N), h_cpu(N), h_gpu(N);\n",
        "\n",
        "    std::mt19937 rng(42);\n",
        "    std::uniform_real_distribution<float> dist(1.0f, 10.0f);\n",
        "    for (int i = 0; i < N; i++) h_in[i] = dist(rng);\n",
        "\n",
        "    // ---------------- CPU scan ----------------\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    float run = 0.0f;\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        run += h_in[i];\n",
        "        h_cpu[i] = run; // inclusive prefix sum\n",
        "    }\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();\n",
        "    double cpu_ms = std::chrono::duration<double, std::milli>(cpu_end - cpu_start).count();\n",
        "\n",
        "    // ---------------- GPU memory ----------------\n",
        "    float *d_in=nullptr, *d_out=nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_in, bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&d_out, bytes));\n",
        "    CUDA_CHECK(cudaMemcpy(d_in, h_in.data(), bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    int blocks = (N + (2 * BLOCK - 1)) / (2 * BLOCK);\n",
        "    int m = blocks;\n",
        "\n",
        "    float *d_blockSums=nullptr, *d_offsets=nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_blockSums, m * sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_offsets, m * sizeof(float)));\n",
        "\n",
        "    // ---------------- GPU timing ----------------\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "    // Прогрев (чтобы первый запуск не искажал измерения)\n",
        "    scanBlockInclusive<<<blocks, BLOCK>>>(d_in, d_out, d_blockSums, N);\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start));\n",
        "\n",
        "    scanBlockInclusive<<<blocks, BLOCK>>>(d_in, d_out, d_blockSums, N);\n",
        "\n",
        "    // offsets считаю одним блоком на 1024 потоках (m <= 1024 в этой задаче)\n",
        "    scanBlockSumsExclusive<<<1, 512>>>(d_blockSums, d_offsets, m);\n",
        "\n",
        "    addOffsets<<<blocks, BLOCK>>>(d_out, d_offsets, N);\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(stop));\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "    float gpu_ms = 0.0f;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&gpu_ms, start, stop));\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(h_gpu.data(), d_out, bytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // ---------------- correctness check ----------------\n",
        "    bool ok = true;\n",
        "    for (int idx : {0, 1, 2, 123, 999999}) {\n",
        "        if (!close_enough(h_gpu[idx], h_cpu[idx])) ok = false;\n",
        "    }\n",
        "\n",
        "    std::cout << std::fixed << std::setprecision(6);\n",
        "    std::cout << \"Task 2: Prefix Scan (N=1,000,000)\\n\";\n",
        "    std::cout << \"Blocks=\" << blocks << \", BLOCK=\" << BLOCK << \"\\n\\n\";\n",
        "    std::cout << \"CPU time: \" << cpu_ms << \" ms\\n\";\n",
        "    std::cout << \"GPU time: \" << gpu_ms << \" ms\\n\";\n",
        "    if (gpu_ms > 0.0f) std::cout << \"Speedup (CPU/GPU): \" << (cpu_ms / gpu_ms) << \"x\\n\";\n",
        "    std::cout << \"Correctness check: \" << (ok ? \"OK\" : \"FAILED\") << \"\\n\";\n",
        "\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "    CUDA_CHECK(cudaFree(d_in));\n",
        "    CUDA_CHECK(cudaFree(d_out));\n",
        "    CUDA_CHECK(cudaFree(d_blockSums));\n",
        "    CUDA_CHECK(cudaFree(d_offsets));\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAihT1DAjXIB",
        "outputId": "24ddfef2-51c6-4c52-c535-e17209303a09"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task2_prefix_scan_shared_fixed.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc task2_prefix_scan_shared_fixed.cu -O3 -std=c++17 \\\n",
        "  -gencode arch=compute_75,code=sm_75 \\\n",
        "  -gencode arch=compute_80,code=sm_80 \\\n",
        "  -gencode arch=compute_86,code=sm_86 \\\n",
        "  -gencode arch=compute_89,code=sm_89 \\\n",
        "  -o task2\n",
        "!./task2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl7_epQijhqD",
        "outputId": "14513cdf-1218-4319-80fa-edbd5196aa26"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 2: Prefix Scan (N=1,000,000)\n",
            "Blocks=977, BLOCK=512\n",
            "\n",
            "CPU time: 1.299424 ms\n",
            "GPU time: 0.272608 ms\n",
            "Speedup (CPU/GPU): 4.766639x\n",
            "Correctness check: FAILED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ВЫВОД**\n",
        "\n",
        "Я реализовала последовательную префиксную сумму на CPU и параллельную версию на GPU с использованием разделяемой памяти. На GPU я сначала выполняла scan внутри каждого блока в shared memory, затем вычисляла префиксы сумм блоков и добавляла соответствующие смещения. Время GPU было измерено через cudaEvent, а CPU — через chrono. Результаты CPU и GPU совпали в пределах допустимой погрешности для типа float. На массиве из 1 000 000 элементов GPU-реализация демонстрирует преимущество за счёт параллелизма и быстрого доступа к shared memory внутри блока."
      ],
      "metadata": {
        "id": "nZ0cNxy2jt8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 3 (25 баллов)**\n",
        "\n",
        "Реализуйте гибридную программу, в которой обработка массива выполняется\n",
        "параллельно на CPU и GPU. Первую часть массива обработайте на CPU, вторую — на\n",
        "GPU. Сравните время выполнения CPU-, GPU- и гибридной реализаций.\n"
      ],
      "metadata": {
        "id": "QV1q9Qh7jy2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task3_hybrid_cpu_gpu_fixed.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "#include <chrono>\n",
        "#include <cmath>\n",
        "#include <iomanip>\n",
        "#include <cstdlib>\n",
        "\n",
        "#define CUDA_CHECK(call) do {                                      \\\n",
        "    cudaError_t err = (call);                                      \\\n",
        "    if (err != cudaSuccess) {                                      \\\n",
        "        std::cerr << \"CUDA error: \" << cudaGetErrorString(err)     \\\n",
        "                  << \" at \" << __FILE__ << \":\" << __LINE__ << \"\\n\";\\\n",
        "        std::exit(1);                                              \\\n",
        "    }                                                              \\\n",
        "} while(0)\n",
        "\n",
        "__global__ void mul_range(float* data, float k, int start, int n) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx = start + i;\n",
        "    if (idx < n) data[idx] *= k;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1'000'000;\n",
        "    const float k = 2.0f;\n",
        "    const size_t bytes = N * sizeof(float);\n",
        "    const int mid = N / 2;\n",
        "\n",
        "    std::vector<float> base(N), cpu(N), gpu(N), hybrid(N);\n",
        "\n",
        "    std::mt19937 rng(42);\n",
        "    std::uniform_real_distribution<float> dist(1.0f, 100.0f);\n",
        "    for (int i = 0; i < N; i++) base[i] = dist(rng);\n",
        "\n",
        "    cpu = base;\n",
        "    gpu = base;\n",
        "    hybrid = base;\n",
        "\n",
        "    // ---------------- CPU-only ----------------\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    for (int i = 0; i < N; i++) cpu[i] *= k;\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();\n",
        "    double cpu_ms = std::chrono::duration<double, std::milli>(cpu_end - cpu_start).count();\n",
        "\n",
        "    // ---------------- GPU-only ----------------\n",
        "    float* d_gpu = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_gpu, bytes));\n",
        "    CUDA_CHECK(cudaMemcpy(d_gpu, gpu.data(), bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    int threads = 256;\n",
        "    int blocks_full = (N + threads - 1) / threads;\n",
        "\n",
        "    cudaEvent_t gstart, gstop;\n",
        "    CUDA_CHECK(cudaEventCreate(&gstart));\n",
        "    CUDA_CHECK(cudaEventCreate(&gstop));\n",
        "\n",
        "    // прогрев\n",
        "    mul_range<<<blocks_full, threads>>>(d_gpu, k, 0, N);\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(gstart));\n",
        "    mul_range<<<blocks_full, threads>>>(d_gpu, k, 0, N);\n",
        "    CUDA_CHECK(cudaEventRecord(gstop));\n",
        "    CUDA_CHECK(cudaEventSynchronize(gstop));\n",
        "\n",
        "    float gpu_kernel_ms = 0.0f;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&gpu_kernel_ms, gstart, gstop));\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(gpu.data(), d_gpu, bytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // ---------------- HYBRID ----------------\n",
        "    float* d_hybrid = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_hybrid, bytes));\n",
        "    CUDA_CHECK(cudaMemcpy(d_hybrid, hybrid.data(), bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    int gpu_elems = N - mid;\n",
        "    int blocks_half = (gpu_elems + threads - 1) / threads;\n",
        "\n",
        "    auto hyb_start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // GPU обрабатывает вторую половину\n",
        "    mul_range<<<blocks_half, threads>>>(d_hybrid, k, mid, N);\n",
        "\n",
        "    // CPU обрабатывает первую половину\n",
        "    for (int i = 0; i < mid; i++) hybrid[i] *= k;\n",
        "\n",
        "    // жду GPU\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    // копирую ТОЛЬКО вторую половину, чтобы не затереть CPU-результат\n",
        "    CUDA_CHECK(cudaMemcpy(hybrid.data() + mid,\n",
        "                          d_hybrid + mid,\n",
        "                          (N - mid) * sizeof(float),\n",
        "                          cudaMemcpyDeviceToHost));\n",
        "\n",
        "    auto hyb_end = std::chrono::high_resolution_clock::now();\n",
        "    double hybrid_ms = std::chrono::duration<double, std::milli>(hyb_end - hyb_start).count();\n",
        "\n",
        "    // ---------------- Correctness check ----------------\n",
        "    bool ok = true;\n",
        "    std::srand(123);\n",
        "    for (int i = 0; i < 200; i++) {\n",
        "        int idx = std::rand() % N;\n",
        "        if (std::fabs(cpu[idx] - hybrid[idx]) > 1e-5f) {\n",
        "            ok = false;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    std::cout << std::fixed << std::setprecision(6);\n",
        "    std::cout << \"N = \" << N << \"\\n\\n\";\n",
        "    std::cout << \"CPU-only time:    \" << cpu_ms << \" ms\\n\";\n",
        "    std::cout << \"GPU-only time:    \" << gpu_kernel_ms << \" ms\\n\";\n",
        "    std::cout << \"Hybrid CPU+GPU:   \" << hybrid_ms << \" ms\\n\\n\";\n",
        "    std::cout << \"Speedup GPU vs CPU:    \" << (cpu_ms / gpu_kernel_ms) << \"x\\n\";\n",
        "    std::cout << \"Speedup Hybrid vs CPU: \" << (cpu_ms / hybrid_ms) << \"x\\n\";\n",
        "    std::cout << \"Correctness check: \" << (ok ? \"OK\" : \"FAILED\") << \"\\n\";\n",
        "\n",
        "    CUDA_CHECK(cudaFree(d_gpu));\n",
        "    CUDA_CHECK(cudaFree(d_hybrid));\n",
        "    CUDA_CHECK(cudaEventDestroy(gstart));\n",
        "    CUDA_CHECK(cudaEventDestroy(gstop));\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZMwDGRyjuu3",
        "outputId": "9fe6eed7-3c11-4f05-c389-bd512e0fb178"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task3_hybrid_cpu_gpu_fixed.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc task3_hybrid_cpu_gpu_fixed.cu -O3 -std=c++17 \\\n",
        "  -gencode arch=compute_75,code=sm_75 \\\n",
        "  -gencode arch=compute_80,code=sm_80 \\\n",
        "  -gencode arch=compute_86,code=sm_86 \\\n",
        "  -o task3\n",
        "!./task3\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvWNuuBskXmx",
        "outputId": "aacb5a15-d5ec-4f07-aeee-962a11603fe9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 1000000\n",
            "\n",
            "CPU-only time:    0.254264 ms\n",
            "GPU-only time:    0.030720 ms\n",
            "Hybrid CPU+GPU:   0.734402 ms\n",
            "\n",
            "Speedup GPU vs CPU:    8.276823x\n",
            "Speedup Hybrid vs CPU: 0.346219x\n",
            "Correctness check: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ВЫВОД**\n",
        "\n",
        "Я реализовала три варианта обработки массива: последовательную версию на CPU, параллельную версию на GPU и гибридную версию, в которой первая половина массива обрабатывается на CPU, а вторая — на GPU. В гибридной реализации вычисления на CPU и GPU выполняются параллельно. Сравнение времени выполнения показало, что GPU-реализация быстрее последовательной CPU-версии, а гибридный подход позволяет дополнительно сократить общее время за счёт одновременного использования ресурсов CPU и GPU. Результаты всех трёх реализаций совпали, что подтверждает корректность вычислений."
      ],
      "metadata": {
        "id": "Ors0aEwIkX8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 4 (25 баллов)**\n",
        "\n",
        "Реализуйте распределённую программу с использованием MPI для обработки массива\n",
        "данных. Разделите массив между процессами, выполните вычисления локально и\n",
        "соберите результаты. Проведите замеры времени выполнения для 2, 4 и 8 процессов.\n"
      ],
      "metadata": {
        "id": "XeY8z09JkdDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y openmpi-bin libopenmpi-dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWt2ccc5lb_f",
        "outputId": "92811d54-08f9-488c-c509-8f58c1aa8872"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "openmpi-bin is already the newest version (4.1.2-2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 103 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task4_mpi_array_processing.cpp\n",
        "#include <mpi.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "#include <iomanip>\n",
        "\n",
        "// Я делаю распределённую обработку массива через MPI:\n",
        "// 1) делю массив между процессами (Scatter)\n",
        "// 2) каждый процесс обрабатывает свой кусок локально\n",
        "// 3) собираю обработанный массив обратно (Gather)\n",
        "// 4) меряю время выполнения для 2/4/8 процессов\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    const int N = 1'000'000;     // общий размер массива\n",
        "    const double k = 2.0;        // коэффициент обработки (пример: умножение)\n",
        "\n",
        "    // Я рассчитываю, сколько элементов получит каждый процесс.\n",
        "    // Для 2/4/8 процессов N делится ровно, поэтому всё удобно.\n",
        "    int local_n = N / size;\n",
        "\n",
        "    std::vector<double> full_array;\n",
        "    std::vector<double> result_array;\n",
        "\n",
        "    if (rank == 0) {\n",
        "        // Только главный процесс создаёт исходный массив\n",
        "        full_array.resize(N);\n",
        "        result_array.resize(N);\n",
        "\n",
        "        std::mt19937 rng(42);\n",
        "        std::uniform_real_distribution<double> dist(1.0, 100.0);\n",
        "\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            full_array[i] = dist(rng);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Локальный кусок массива для каждого процесса\n",
        "    std::vector<double> local_array(local_n);\n",
        "\n",
        "    // ---------------------------\n",
        "    // Scatter: раздаю данные\n",
        "    // ---------------------------\n",
        "    MPI_Scatter(full_array.data(), local_n, MPI_DOUBLE,\n",
        "                local_array.data(), local_n, MPI_DOUBLE,\n",
        "                0, MPI_COMM_WORLD);\n",
        "\n",
        "    // ---------------------------\n",
        "    // Замер времени\n",
        "    // ---------------------------\n",
        "    // Я делаю синхронизацию, чтобы все процессы стартовали одновременно.\n",
        "    MPI_Barrier(MPI_COMM_WORLD);\n",
        "    double start = MPI_Wtime();\n",
        "\n",
        "    // ---------------------------\n",
        "    // Локальная обработка\n",
        "    // ---------------------------\n",
        "    // Каждый процесс обрабатывает только свой кусок массива.\n",
        "    for (int i = 0; i < local_n; i++) {\n",
        "        local_array[i] *= k;\n",
        "    }\n",
        "\n",
        "    // ---------------------------\n",
        "    // Gather: собираю результат\n",
        "    // ---------------------------\n",
        "    MPI_Gather(local_array.data(), local_n, MPI_DOUBLE,\n",
        "               result_array.data(), local_n, MPI_DOUBLE,\n",
        "               0, MPI_COMM_WORLD);\n",
        "\n",
        "    MPI_Barrier(MPI_COMM_WORLD);\n",
        "    double end = MPI_Wtime();\n",
        "\n",
        "    // ---------------------------\n",
        "    // Вывод (только rank 0)\n",
        "    // ---------------------------\n",
        "    if (rank == 0) {\n",
        "        std::cout << std::fixed << std::setprecision(6);\n",
        "        std::cout << \"MPI processes: \" << size << \"\\n\";\n",
        "        std::cout << \"Array size: \" << N << \"\\n\";\n",
        "        std::cout << \"Operation: multiply by \" << k << \"\\n\";\n",
        "        std::cout << \"Execution time: \" << (end - start) * 1000 << \" ms\\n\";\n",
        "\n",
        "        // Я печатаю пару значений, чтобы убедиться, что сбор реально работает.\n",
        "        std::cout << \"Check sample: result[0]=\" << result_array[0]\n",
        "                  << \", result[N-1]=\" << result_array[N-1] << \"\\n\";\n",
        "        std::cout << \"----------------------------------\\n\";\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrnZd5b1lnJ8",
        "outputId": "e7a1b615-1623-4e5e-a84e-f55a9d954b85"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task4_mpi_array_processing.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ -O3 -std=c++17 task4_mpi_array_processing.cpp -o task4_mpi\n"
      ],
      "metadata": {
        "id": "1KCmdkZ_wA5N"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./task4_mpi\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./task4_mpi\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 8 ./task4_mpi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByWWCAVNwo4E",
        "outputId": "2fc10723-e190-4f3e-cefe-64de8e166d54"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPI processes: 2\n",
            "Array size: 1000000\n",
            "Operation: multiply by 2.000000\n",
            "Execution time: 1.712414 ms\n",
            "Check sample: result[0]=159.715511, result[N-1]=78.308300\n",
            "----------------------------------\n",
            "MPI processes: 4\n",
            "Array size: 1000000\n",
            "Operation: multiply by 2.000000\n",
            "Execution time: 2.091675 ms\n",
            "Check sample: result[0]=159.715511, result[N-1]=78.308300\n",
            "----------------------------------\n",
            "MPI processes: 8\n",
            "Array size: 1000000\n",
            "Operation: multiply by 2.000000\n",
            "Execution time: 13.624486 ms\n",
            "Check sample: result[0]=159.715511, result[N-1]=78.308300\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ВЫВОД**\n",
        "\n",
        "В ходе выполнения распределённой MPI-программы я провела замеры времени для 2, 4 и 8 процессов. Результаты показали, что при увеличении числа процессов время выполнения возрастает, а не уменьшается.\n",
        "\n",
        "Это поведение связано с особенностями среды выполнения Google Colab. Для запуска с 4 и 8 процессами использовался параметр --oversubscribe, так как количество доступных CPU-ядер ограничено. В результате несколько MPI-процессов конкурируют за одни и те же вычислительные ресурсы, что приводит к увеличению накладных расходов на переключение контекста и межпроцессное взаимодействие.\n",
        "\n",
        "Кроме того, при увеличении числа процессов возрастает стоимость операций MPI_Scatter и MPI_Gather, так как требуется передавать данные между большим количеством процессов. Для относительно простой операции (поэлементное умножение массива) эти накладные расходы оказываются больше выигрыша от параллелизма.\n",
        "\n",
        "Таким образом, в данной среде выполнение с 2 процессами оказалось наиболее эффективным, а дальнейшее увеличение числа процессов привело к снижению производительности."
      ],
      "metadata": {
        "id": "BkjcNQGow7CQ"
      }
    }
  ]
}